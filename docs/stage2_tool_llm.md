# Stage II: Tool-Enhanced Large Language Models
This paradigm typically involves two key components: generating actions from the current state, and transitioning between states by executing those actions.


---

### ðŸ“„ [ViperGPT](https://openaccess.thecvf.com/content/ICCV2023/papers/Suris_ViperGPT_Visual_Inference_via_Python_Execution_for_Reasoning_ICCV_2023_paper.pdf)
- **Title:** *ViperGPT: Visual Inference via Python Execution for Reasoning*  
- **Venue:** *ICCV 2023*  
- **GitHub:** [Link](https://github.com/cvlab-columbia/viper) 

<p align="left">
  <img src="https://github.com/cvlab-columbia/viper/raw/main/teaser.gif" alt="ViperGPT Demo" width="500"/>
</p>


---

### ðŸ“„ [Chameleon](https://proceedings.neurips.cc/paper_files/paper/2023/file/871ed095b734818cfba48db6aeb25a62-Paper-Conference.pdf)
- **Title:** *Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models*  
- **Venue:** *NeurIPS 2023*  
- **GitHub:** [Link](https://github.com/lupantech/chameleon-llm) 

<p align="left">
  <img src="https://github.com/lupantech/chameleon-llm/raw/main/assets/showcase_scienceqa.png" alt="Chameleon Demo" width="500"/>
</p>

---

### ðŸ“„ [Visprog](https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.pdf)
- **Title:** *Visual Programming: Compositional visual reasoning without training*  
- **Venue:** *CVPR 2023*  
- **GitHub:** [Link](https://github.com/allenai/visprog) 

<p align="left">
  <img src="https://github.com/allenai/visprog/raw/main/assets/teaser2.png" alt="Visprog Demo" width="500"/>
</p>

---

### ðŸ“„ [Visual ChatGPT](https://arxiv.org/pdf/2303.04671)
- **Title:** *Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models*  
- **Venue:** *arXiv 2023*  
- **GitHub:** [Link](https://github.com/chenfei-wu/TaskMatrix) 

<p align="left">
  <img src="https://github.com/chenfei-wu/TaskMatrix/raw/main/assets/figure.jpg" alt="Visual ChatGPT Demo" width="500"/>
</p>

---

### ðŸ“„ [HuggingGPT](https://proceedings.neurips.cc/paper_files/paper/2023/file/77c33e6a367922d003ff102ffb92b658-Paper-Conference.pdf)
- **Title:** *HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face*  
- **Venue:** *NeurIPS 2023*  
- **GitHub:** [Link](https://github.com/microsoft/JARVIS) 

<p align="left">
  <img src="https://github.com/microsoft/JARVIS/raw/main/hugginggpt/assets/intro.png" alt="HuggingGPT Demo" width="500"/>
</p>

---

### ðŸ“„ [GPT4Tools](https://proceedings.neurips.cc/paper_files/paper/2023/file/e393677793767624f2821cec8bdd02f1-Paper-Conference.pdf)
- **Title:** *GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction*  
- **Venue:** *NeurIPS 2023*  
- **GitHub:** [Link](https://github.com/AILab-CVC/GPT4Tools) 

<p align="left">
  <img src="https://github.com/AILab-CVC/GPT4Tools/raw/master/asserts/images/overview.png" alt="GPT4Tools Demo" width="500"/>
</p>

---

### ðŸ“„ [InternGPT](https://arxiv.org/pdf/2305.05662)
- **Title:** *InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language*  
- **Venue:** *arXiv 2023*  
- **GitHub:** [Link](https://github.com/OpenGVLab/InternGPT) 

<p align="left">
  <img src="https://github.com/OpenGVLab/InternGPT/raw/main/assets/arch1.png" alt="InternGPT Demo" width="500"/>
</p>

---

### ðŸ“„ [ViotGPT](https://ojs.aaai.org/index.php/AAAI/article/view/33160)
- **Title:** *VIoTGPT: Learning to Schedule Vision Tools Towards Intelligent Video Internet of Things*  
- **Venue:** *AAAI 2025*  
- **GitHub:** [Link](https://github.com/zhongyy/VIoTGPT) 

---

### ðŸ“„ [MM-REACT](https://arxiv.org/pdf/2303.11381)
- **Title:** *MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action*  
- **Venue:** *arXiv 2023*  
- **GitHub:** [Link](https://github.com/microsoft/MM-REACT) 

<p align="left">
  <img src="https://camo.githubusercontent.com/d89cec19c13ca8fc448f7bdcccd5c0f0f0bdbd94cf5cfe4948ab96e4bd5119f3/68747470733a2f2f6d756c74696d6f64616c2d72656163742e6769746875622e696f2f696d616765732f7465617365722e706e67" alt="MM-REACT Demo" width="500"/>
</p>

---

### ðŸ“„ [VisRep](https://openaccess.thecvf.com/content/CVPR2024/html/Khan_Self-Training_Large_Language_Models_for_Improved_Visual_Program_Synthesis_With_CVPR_2024_paper.html)
- **Title:** *Self-training large language models for improved visual program synthesis with visual reinforcement*  
- **Venue:** *CVPR 2024*  

<p align="left">
  <img src="https://zaidkhan.me/ViReP/static/images/CVPR2024-teaser.png" alt="VisRep Demo" width="500"/>
</p>

---

### ðŸ“„ [CRAFT](https://openreview.net/forum?id=G0vdDSt9XM)
- **Title:** *CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets*  
- **Venue:** *ICLR 2024*  
- **GitHub:** [Link](https://github.com/lifan-yuan/CRAFT) 

<p align="left">
  <img src="https://github.com/lifan-yuan/CRAFT/raw/main/docs/CRAFT.jpg" alt="CRAFT Demo" width="500"/>
</p>

---

### ðŸ“„ [CLOVA](https://openaccess.thecvf.com/content/CVPR2024/papers/Gao_CLOVA_A_Closed-LOop_Visual_Assistant_with_Tool_Usage_and_Update_CVPR_2024_paper.pdf)
- **Title:** *CLOVA: A Closed-LOop Visual Assistant with Tool Usage and Update*  
- **Venue:** *CVPR 2024*  
- **GitHub:** [Link](https://github.com/clova-tool/CLOVA-tool) 

<p align="left">
  <img src="https://github.com/clova-tool/CLOVA-tool/raw/master/figure/illustration_examples_framework.png" alt="CLOVA Demo" width="500"/>
</p>

---

### ðŸ“„ [HYDRA](https://hydra-vl4ai.github.io/)
- **Title:** *HYDRA*  
- **Venue:** *ECCV 2024*  
- **GitHub:** [Link](https://github.com/ControlNet/HYDRA) 

<p align="left">
  <img src="https://github.com/ControlNet/HYDRA/raw/master/media/Frame.png" alt="HYDRA Demo" width="500"/>
</p>

---

### ðŸ“„ [ContextualCoder](https://ieeexplore.ieee.org/document/10891469)
- **Title:** *ContextualCoder: Adaptive In-context Prompting for Programmatic Visual Question Answering*  
- **Venue:** *TMM 2025*  


---

### ðŸ“„ [ViUniT](https://openaccess.thecvf.com/content/CVPR2025/papers/Panagopoulou_ViUniT_Visual_Unit_Tests_for_More_Robust_Visual_Programming_CVPR_2025_paper.pdf)
- **Title:** *Visual Unit Tests for More Robust Visual Programming*  
- **Venue:** *CVPR 2025*  
- **GitHub:** [Link](https://github.com/SalesforceAIResearch/visual-unit-testing) 

<p align="left">
  <img src="https://github.com/SalesforceAIResearch/visual-unit-testing/raw/main/assets/ViUnit%20Demo.png" alt="ViUniT Demo" width="500"/>
</p>

---

### ðŸ“„ [SYNAPSE](https://ojs.aaai.org/index.php/AAAI/article/view/34965)
- **Title:** *SYNAPSE: SYmbolic Neural-Aided Preference Synthesis Engine*  
- **Venue:** *AAAI 2025*  
- **GitHub:** [Link](https://github.com/ut-amrl/synapse) 

<p align="left">
  <img src="https://github.com/ut-amrl/synapse/raw/main/assets/framework.png" alt="SYNAPSE Demo" width="500"/>
</p>

---

### ðŸ“„ [Naver](https://ojs.aaai.org/index.php/AAAI/article/view/34965)
- **Title:** *NAVER: A Neuro-Symbolic Compositional Automaton for Visual Grounding with Explicit Logic Reasoning*  
- **Venue:** *ICCV 2025*  
- **GitHub:** [Link](https://github.com/ControlNet/NAVER) 

<p align="left">
  <img src="https://github.com/ControlNet/NAVER/raw/master/assets/teaser.svg" alt="Naver Demo" width="500"/>
</p>

---

### ðŸ“„ DWIM
- **Title:** *DWIM: Towards Tool-aware Visual Reasoning via Discrepancy-aware Workflow Generation & Instruct-Masking Tuning*  
- **Venue:** *ICCV 2025*  
- **GitHub:** [Link](https://github.com/pokerme7777/DWIM/) 
- **Website:** [Link](https://pokerme7777.github.io/DWIM.github.io/) 

<p align="left">
  <img src="https://github.com/pokerme7777/DWIM/raw/main/assets/Method.svg" alt="DWIM Demo" width="500"/>
</p>

---

### ðŸ“„ [LEFT](https://proceedings.neurips.cc/paper_files/paper/2023/file/79fea214543ba263952ac3f4e5452b14-Paper-Conference.pdf)
- **Title:** *Whatâ€™s Left? Concept Grounding with Logic-Enhanced Foundation Models*  
- **Venue:** *NeurIPS 2023*  
- **GitHub:** [Link](https://github.com/joyhsu0504/LEFT/) 

<p align="left">
  <img src="https://github.com/joyhsu0504/LEFT/raw/main/figure.png" alt="LEFT Demo" width="500"/>
</p>