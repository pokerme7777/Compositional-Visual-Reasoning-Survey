# Stage I: Prompt-Enhanced Language-Centric

This stage focuses on methods where LLMs act as the central reasoning engine, guided by textual prompts. Visual inputs are often pre-processed or summarized before being fed into the LLM.

---

## ðŸ”¹ Category 1: Question Decomposition â†’ Perception â†’ Answer

These methods prompt the LLM to break down complex visual questions into sub-questions and resolve them sequentially.


---

### ðŸ“„ [DDCoT](https://openreview.net/forum?id=ktYjrgOENR)
- **Title:** *DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models*
- **Venue:** *NIPS 2023*
- **GitHub:** [Link](https://github.com/SooLab/DDCOT)

  <img src="https://github.com/YangBin55/DDCOT/raw/master/images/teaser.png" alt="DDCoT Demo"  width="500"/>


---

### ðŸ“„ [IdealGPT](https://arxiv.org/pdf/2305.14985)
- **Title:** *IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models*
- **Venue:** *EMNLP 2023*
- **GitHub:** [Link](https://github.com/Hxyou/IdealGPT)

  <img src="https://github.com/Hxyou/IdealGPT/raw/master/figs/main_diagram.jpg" alt="IdealGPT Demo"  width="500"/>

---

### ðŸ“„ [Modeling Collaborator](https://openaccess.thecvf.com/content/CVPR2024/papers/Toubal_Modeling_Collaborator_Enabling_Subjective_Vision_Classification_With_Minimal_Human_Effort_CVPR_2024_paper.pdf)
- **Title:** *Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use*  
- **Venue:** *CVPR 2024*  


---

### ðŸ“„ [ChatCaptioner](https://openreview.net/forum?id=1LoVwFkZNo)
- **Title:** *ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions*  
- **Venue:** *TMLR 2023*  
- **GitHub:** [Link](https://github.com/Vision-CAIR/ChatCaptioner)   
<img src="https://github.com/Vision-CAIR/ChatCaptioner/raw/main/ChatCaptioner/demo_pic/demo1.gif" alt="ChatCaptioner Demo" width="300"/>


## ðŸ”¹ Category 2: Perception â†’ Answer
These methods rely on visual perception modules (typically VLMs) to extract relevant information from the image. The LLM is then prompted to directly generate the final answer based on this perceived information, without explicit intermediate reasoning or decomposition.

---
### ðŸ“„ [Cola](https://proceedings.neurips.cc/paper_files/paper/2023/hash/ddfe6bae7b869e819f842753009b94ad-Abstract-Conference.html)
- **Title:** *Large Language Models are Visual Reasoning Coordinators*  
- **Venue:** *NeurIPS 2023*  
- **GitHub:** [Link](https://github.com/cliangyu/Cola)   
<img src="https://camo.githubusercontent.com/1ba90ea07891c3659e5938b00fe1c9447269bec692cdc98f8f13da6f0cbdfbf0/68747470733a2f2f692e706f7374696d672e63632f5a7158536e38724e2f736d2d7465617365722e706e67" alt="cola Demo" width="400"/>

---

### ðŸ“„ [MM-CoT](https://arxiv.org/pdf/2302.00923)
- **Title:** *Multimodal Chain-of-Thought Reasoning in Language Models*  
- **Venue:** *TMLR 2024*  
- **GitHub:** [Link](https://github.com/amazon-science/mm-cot)  
<img src="https://github.com/amazon-science/mm-cot/raw/main/vision_features/mm-cot.png" alt="MM-CoT Demo"/>

---

### ðŸ“„ [AdGPT](https://dl.acm.org/doi/pdf/10.1145/3720546)
- **Title:** *AdGPT: Explore Meaningful Advertising with ChatGPT*  
- **Venue:** *ACM MM 2025*  
- **GitHub:** [Link](https://github.com/Rbrq03/AdGPT)  
<img src="https://github.com/Rbrq03/AdGPT/raw/main/assert/figure1.png" alt="AdGPT Demo" width="500"/>


---

### ðŸ“„ [PROMPTCAP](https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_PromptCap_Prompt-Guided_Image_Captioning_for_VQA_with_GPT-3_ICCV_2023_paper.pdf)
- **Title:** *PromptCap: Prompt-Guided Task-Aware Image Captioning*  
- **Venue:** *ICCV 2023*  
- **GitHub:** [Link](https://github.com/Yushi-Hu/PromptCap)   
<img src="https://yushi-hu.github.io/promptcap_demo/static/images/promptcap_teaser.png" alt="PROMPTCAP Demo" width="300"/>

---

### ðŸ“„ [VCTP](https://ojs.aaai.org/index.php/AAAI/article/view/27888)
- **Title:** *Visual Chain-of-Thought Prompting for Knowledge-Based Visual Reasoning*  
- **Venue:** *AAAI 2024*  
- **GitHub:** [Link](https://github.com/UMass-Embodied-AGI/VisualCoT)   
<img src="https://github.com/UMass-Embodied-AGI/VisualCoT/raw/main/framework.png" alt="VCTP Demo"/>


---


### ðŸ“„ [Finedefics](https://openreview.net/forum?id=p3NKpom1VL)
- **Title:** *Analyzing and Boosting the Power of Fine-Grained Visual Recognition for Multi-modal Large Language Models*  
- **Venue:** *ICLR 2025*  
- **GitHub:** [Link](https://github.com/PKU-ICST-MIPL/Finedefics_ICLR2025)
  
  <img src="https://iclr.cc/media/PosterPDFs/ICLR%202025/28323.png?t=1744547699.8079863" alt="finedefics Demo" width="500"/>


