# Stage IV: Chain-of-Thought Vision Language Models

These models carry out multi-step reasoning in a single forward pass without external tools, explicitly exposing intermediate reasoning and perceptual states before the final answer.

---
### ðŸ“„ [LLaVA-CoT](https://arxiv.org/pdf/2411.10440)
- **Title:** *LLaVA-CoT: Let Vision Language Models Reason Step-by-Step*  
- **Venue:** *arXiv 2024*  
- **GitHub:** [Link](https://github.com/PKU-YuanGroup/LLaVA-CoT) 

---

### ðŸ“„ [CCoT](https://openaccess.thecvf.com/content/CVPR2024/papers/Mitra_Compositional_Chain-of-Thought_Prompting_for_Large_Multimodal_Models_CVPR_2024_paper.pdf)
- **Title:** *Compositional Chain-of-Thought Prompting for Large Multimodal Models*  
- **Venue:** *CVPR 2024*  
- **GitHub:** [Link](https://github.com/chancharikmitra/CCoT) 

<p align="left">
  <img src="https://github.com/chancharikmitra/CCoT/raw/main/images/fig1_v7.png" alt="CCoT Demo" width="300"/>
</p>

---

### ðŸ“„ [PaLI](https://arxiv.org/abs/2312.03052)
- **Title:** *Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models*  
- **Venue:** *CVPR 2024*  


---

### ðŸ“„ [VOCoT](https://aclanthology.org/2025.naacl-long.192.pdf)
- **Title:** *VoCoT: Unleashing Visually Grounded Multi-Step Reasoning in Large Multi-Modal Models*  
- **Venue:** *NAACL 2025*  
- **GitHub:** [Link](https://github.com/RupertLuo/VoCoT) 

<p align="left">
  <img src="https://github.com/RupertLuo/VoCoT/raw/main/figs/model_arch.png" alt="VOCoT Demo" width="500"/>
</p>

---

### ðŸ“„ [MM-GCoT](https://arxiv.org/pdf/2503.12799)
- **Title:** *Grounded Chain-of-Thought for Multimodal Large Language Models*  
- **Venue:** *arXiv 2025*  
- **GitHub:** [Link](https://github.com/DoubtedSteam/MM-GCoT) 

---

### ðŸ“„ [LLaVA-Aurora](https://openaccess.thecvf.com/content/CVPR2025/papers/Bigverdi_Perception_Tokens_Enhance_Visual_Reasoning_in_Multimodal_Language_Models_CVPR_2025_paper.pdf)
- **Title:** *Perception Tokens Enhance Visual Reasoning in Multimodal Language Models*  
- **Venue:** *CVPR 2025*  
- **GitHub:** [Link](https://github.com/mahtabbigverdi/Aurora-perception) 

<p align="left">
  <img src="https://github.com/mahtabbigverdi/Aurora-perception/raw/main/assets/intro_figure.png" alt="LLaVA-Aurora Demo" width="500"/>
</p>

---

### ðŸ“„ [DeepPerception](https://arxiv.org/pdf/2503.12797)
- **Title:** *DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding*  
- **Venue:** *arXiv 2025*  
- **GitHub:** [Link](https://github.com/thunlp/DeepPerception) 

<p align="left">
  <img src="https://github.com/thunlp/DeepPerception/raw/main/figs/header.png" alt="DeepPerception Demo" width="500"/>
</p>

---

### ðŸ“„ [CoReS](https://arxiv.org/pdf/2404.05673)
- **Title:** *CoReS: Orchestrating the Dance of Reasoning and Segmentation*  
- **Venue:** *ECCV 2024*  
- **GitHub:** [Link](https://github.com/baoxiaoyi/CoReS) 

<p align="left">
  <img src="https://chain-of-reasoning-and-segmentation.github.io/static/images/comparison.png" alt="CoReS Demo" width="500"/>
</p>


---

### ðŸ“„ [G1](https://arxiv.org/abs/2505.13426)
- **Title:** *G1: BOOTSTRAPPING PERCEPTION AND REASONING ABILITIES OF VISION-LANGUAGE MODEL VIA REINFORCEMENT LEARNING*  
- **Venue:** *arXiv 2025*  
- **GitHub:** [Link](https://github.com/chenllliang/G1)

<p align="left">
  <img src="https://github.com/chenllliang/G1/raw/main/assets/image.png" alt="G1 Demo" width="500"/>
</p>

---

### ðŸ“„ [Vision-R1](https://arxiv.org/pdf/2503.06749)
- **Title:** *Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models*  
- **Venue:** *arXiv 2025*  
- **GitHub:** [Link](https://github.com/Osilly/Vision-R1) 

<p align="left">
  <img src="https://github.com/Osilly/Vision-R1/raw/main/figs/interleaving_reasoning.png" alt="Vision-R1 Demo" width="500"/>
</p>

---

### ðŸ“„ [Ground-R1](https://arxiv.org/pdf/2505.20272)
- **Title:** *Ground-R1: Incentivizing Grounded Visual Reasoning via Reinforcement Learning*  
- **Venue:** *arXiv 2025*  
- **GitHub:** [Link]() 

<p align="left">
  <img src="" alt="Ground-R1 Demo" width="500"/>
</p>

---

### ðŸ“„ [Griffon-R](https://arxiv.org/abs/2505.20753)
- **Title:** *Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models*  
- **Venue:** *arXiv 2025*  
- **GitHub:** [Link](https://github.com/zzzhhzzz/Ground-R1) 

<p align="left">
  <img src="https://github.com/zzzhhzzz/Ground-R1/raw/master/asset/teaser01.png" alt="Griffon-R Demo" width="500"/>
</p>

---

### ðŸ“„ [VISTAR](https://openaccess.thecvf.com/content/CVPR2025W/XAI4CV/papers/Cheng_Visually_Interpretable_Subtask_Reasoning_for_Visual_Question_Answering_CVPRW_2025_paper.pdf)
- **Title:** *Visually Interpretable Subtask Reasoning for Visual Question Answering*  
- **Venue:** *CVPR 2025*  
- **GitHub:** [Link](https://github.com/ChengJade/VISTAR) 


---

### ðŸ“„ [CoF](https://arxiv.org/pdf/2505.15436)
- **Title:** *Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL*  
- **Venue:** *arXiv 2025*  
- **GitHub:** [Link](https://github.com/xtong-zhang/Chain-of-Focus) 

<p align="left">
  <img src="https://github.com/xtong-zhang/Chain-of-Focus/raw/main/assets/teaser.jpg" alt="CoF Demo" width="500"/>
</p>

---

### ðŸ“„ [LATTE](https://arxiv.org/pdf/2412.05479)
- **Title:** *LATTE: Learning to Think with Vision Specialists*  
- **Venue:** *Arxiv 2024*  
- **GitHub:** [Link](https://github.com/SalesforceAIResearch/LATTE/) 

<p align="left">
  <img src="https://github.com/SalesforceAIResearch/LATTE/raw/main/image/teaser.png" alt="LATTE Demo" width="500"/>
</p>