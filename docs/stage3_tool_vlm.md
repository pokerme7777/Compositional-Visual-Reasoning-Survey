# Stage III: Tool-Enhanced Vision Language Models

Tool-enhanced VLMs extend tool-enhanced LLMs by replacing the language model with a vision-language model, enabling direct visual interaction. Unlike tool-enhanced LLMs, planners here take raw images as input, reducing information loss and improving efficiency.

---

### ðŸ“„ [Image-of-Thought](https://arxiv.org/pdf/2405.13872)
- **Title:** *Image-of-Thought Prompting for Visual Reasoning Refinement in Multimodal Large Language Models*  
- **Venue:** *arXiv 2024*  


---

### ðŸ“„ [P2G](https://arxiv.org/pdf/2403.19322)
- **Title:** *Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models*  
- **Venue:** *arXiv 2024*  

---

### ðŸ“„ [LLAVA-PLUS](https://openreview.net/forum?id=IB1HqbA2Pn)
- **Title:** *LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents*  
- **Venue:** *ICLR 2024*  
- **GitHub:** [Link](https://github.com/LLaVA-VL/llava-plus) 


---

### ðŸ“„ [VIREO](https://arxiv.org/pdf/2406.19934)
- **Title:** *From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis*  
- **Venue:** *EMNLP 2024*  
- **GitHub:** [Link](https://github.com/steven-ccq/VisualReasoner) 

<p align="left">
  <img src="https://github.com/steven-ccq/VisualReasoner/raw/main/imgs/pipeline.png" alt="VIREO Demo" width="500"/>
</p>

---

### ðŸ“„ [Openthinkimg](https://arxiv.org/pdf/2505.08617)
- **Title:** *OPENTHINKIMG: Learning to Think with Images via Visual Tool Reinforcement Learning*  
- **Venue:** *arXiv 2025*  
- **GitHub:** [Link](https://github.com/zhaochen0110/OpenThinkIMG) 

<p align="left">
  <img src="https://github.com/zhaochen0110/OpenThinkIMG/raw/main/docs/v-toolrl.png" alt="Openthinkimg Demo" width="500"/>
</p>

---

### ðŸ“„ [SKETCHPAD](https://arxiv.org/pdf/2406.09403)
- **Title:** *Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models*  
- **Venue:** *NeurIPS 2024*  
- **GitHub:** [Link](https://github.com/Yushi-Hu/VisualSketchpad) 

<p align="left">
  <img src="https://github.com/Yushi-Hu/VisualSketchpad/raw/main/assets/teaser.jpg" alt="SKETCHPAD Demo" width="500"/>
</p>

---

### ðŸ“„ [VisionLLM v2](https://proceedings.neurips.cc/paper_files/paper/2024/file/81a60d18e010b27b36cd465c6604b915-Paper-Conference.pdf)
- **Title:** *VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks*  
- **Venue:** *NeurIPS 2024*  
- **GitHub:** [Link](https://github.com/OpenGVLab/VisionLLM) 

<p align="left">
  <img src="https://github.com/OpenGVLab/VisionLLM/raw/main/VisionLLMv2/assets/arch.png" alt="VisionLLM v2 Demo" width="500"/>
</p>

---

### ðŸ“„ [VITRON](https://arxiv.org/pdf/2412.19806)
- **Title:** *VITRON: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing*  
- **Venue:** *NeurIPS 2024*  
- **GitHub:** [Link](https://github.com/SkyworkAI/Vitron) 

<p align="left">
  <img src="https://github.com/SkyworkAI/Vitron/raw/main/assets/intro.png" alt="VITRON Demo" width="500"/>
</p>

---

### ðŸ“„ [Syn](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Synthesize_Step-by-Step_Tools_Templates_and_LLMs_as_Data_Generators_for_CVPR_2024_paper.pdf)
- **Title:** *Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA*  
- **Venue:** *CVPR 2024*  


---

### ðŸ“„ [VTool-R1](https://arxiv.org/pdf/2505.19255)
- **Title:** *VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use*  
- **Venue:** *arXiv 2025*  
- **GitHub:** [Link](https://github.com/VTool-R1/VTool-R1) 

<p align="left">
  <img src="https://github.com/VTool-R1/VTool-R1/raw/main/vtool_example.png" alt="VTool-R1 Demo" width="500"/>
</p>

---

### ðŸ“„ [Self-Imagine](https://arxiv.org/pdf/2401.08025)
- **Title:** *Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination*  
- **Venue:** *arXiv 2024*  
- **GitHub:** [Link](https://github.com/snat1505027/self-imagine) 


---

### ðŸ“„ [Creative Agents](https://openreview.net/forum?id=y0dbr5uSc9)
- **Title:** *Creative Agents: Empowering Agents with Imagination for Creative Tasks*  
- **Venue:** *UAI 2025*  
- **GitHub:** [Link](https://github.com/PKU-RL/Creative-Agents) 

<p align="left">
  <img src="https://github.com/PKU-RL/Creative-Agents/raw/main/figs/pipeline.png" alt="Creative Agents Demo" width="500"/>
</p>

---

### ðŸ“„ [CoT-VLA](https://openaccess.thecvf.com/content/CVPR2025/papers/Zhao_CoT-VLA_Visual_Chain-of-Thought_Reasoning_for_Vision-Language-Action_Models_CVPR_2025_paper.pdf)
- **Title:** *CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models*  
- **Venue:** *CVPR 2025*  
- **Website:** [Link](https://cot-vla.github.io/) 

<p align="left">
  <img src="https://cot-vla.github.io/media/cot_vla_pipeline.png" alt="CoT-VLA Demo" width="500"/>
</p>
