# Stage V: Unified Agentic Vision Language Models
These models incorporate higher-order cognitive mechanisms such as planning, memory, operation, imagination, textual feedback and visual evidence.
---
### ðŸ“„ [V*](https://openaccess.thecvf.com/content/CVPR2024/html/Wu_V_Guided_Visual_Search_as_a_Core_Mechanism_in_Multimodal_CVPR_2024_paper.html)
- **Title:** *V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs*  
- **Venue:** *CVPR 2024*  
- **GitHub:** [Link](https://github.com/penghao-wu/vstar) 

<p align="left">
  <img src="https://github.com/penghao-wu/vstar/raw/main/assets/teaser.png" alt="V* Demo" width="500"/>
</p>

---

### ðŸ“„ [DC2](https://ojs.aaai.org/index.php/AAAI/article/view/32852)
- **Title:** *Divide, Conquer and Combine: A Training-Free Framework for High-Resolution Image Perception in Multimodal Large Language Models*  
- **Venue:** *AAAI 2025*  
- **GitHub:** [Link](https://github.com/DreamMr/HR-Bench) 

<p align="left">
  <img src="https://github.com/DreamMr/HR-Bench/raw/main/resources/framework_version_8.png" alt="DC2 Demo" width="500"/>
</p>

---

### ðŸ“„ [ZoomEye](https://arxiv.org/pdf/2411.16044)
- **Title:** *ZoomEye: Enhancing Multimodal LLMs with Human-Like Zooming Capabilities through Tree-Based Image Exploration*  
- **Venue:** *arXiv 2024*  
- **GitHub:** [Link](https://github.com/om-ai-lab/ZoomEye) 

<p align="left">
  <img src="https://github.com/om-ai-lab/ZoomEye/raw/main/docs/examples.jpg" alt="ZoomEye Demo" width="500"/>
</p>

---

### ðŸ“„ [FAST](https://arxiv.org/pdf/2408.08862)
- **Title:** *Visual Agents as Fast and Slow Thinkers*  
- **Venue:** *ICLR 2025*  
- **GitHub:** [Link](https://github.com/GuangyanS/Sys2-LLaVA) 


---

### ðŸ“„ [CogCoM](https://arxiv.org/pdf/2402.04236)
- **Title:** *CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations*  
- **Venue:** *ICLR 2025*  
- **GitHub:** [Link](https://github.com/THUDM/CogCoM) 

<p align="left">
  <img src="https://github.com/THUDM/CogCoM/raw/main/assets/cases.png" alt="CogCoM Demo" width="400"/>
</p>

---

### ðŸ“„ [GeReA](https://arxiv.org/pdf/2402.02503)
- **Title:** *GeReA: Question-Aware Prompt Captions for Knowledge-based Visual Question Answering*  
- **Venue:** *arXiv 2024*  
- **GitHub:** [Link](https://github.com/Upper9527/GeReA) 

<p align="left">
  <img src="https://github.com/Upper9527/GeReA/raw/main/figs/framework.png" alt="GeReA Demo" width="500"/>
</p>

---

### ðŸ“„ [Insight-V](https://github.com/dongyh20/Insight-V)
- **Title:** *Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models*  
- **Venue:** *CVPR 2025*  
- **GitHub:** [Link](https://github.com/dongyh20/Insight-V)

<p align="left">
  <img src="https://github.com/dongyh20/Insight-V/raw/master/figure/data-generation.png" alt="Insight-V Demo" width="500"/>
</p>

---

### ðŸ“„ [Argus](https://openaccess.thecvf.com/content/CVPR2025/papers/Man_Argus_Vision-Centric_Reasoning_with_Grounded_Chain-of-Thought_CVPR_2025_paper.pdf)
- **Title:** *Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought*  
- **Venue:** *CVPR 2025*  

---

### ðŸ“„ [Mirage](https://arxiv.org/pdf/2506.17218)
- **Title:** *Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens*  
- **Venue:** *arXiv 2025*  
- **GitHub:** [Link](https://github.com/UMass-Embodied-AGI/Mirage) 

<p align="left">
  <img src="https://github.com/UMass-Embodied-AGI/Mirage/raw/main/asset/teaser.png" alt="Mirage Demo" width="500"/>
</p>

---

### ðŸ“„ [FOREWARN](https://www.roboticsproceedings.org/rss21/p076.pdf)
- **Title:** *From Foresight to Forethought: VLM-In-the-Loop Policy Steering via Latent Alignment*  
- **Venue:** *RSS 2025*  
- **GitHub:** [Link](https://yilin-wu98.github.io/forewarn/) 

<p align="left">
  <img src="https://yilin-wu98.github.io/forewarn/static/figures/architecture_figure.png" alt="FOREWARN Demo" width="500"/>
</p>

---

### ðŸ“„ [VisCoT](https://arxiv.org/pdf/2403.16999)
- **Title:** *Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning*  
- **Venue:** *NeurIPS 2024*  
- **GitHub:** [Link](https://github.com/deepcs233/Visual-CoT/) 

<p align="left">
  <img src="https://github.com/deepcs233/Visual-CoT/raw/main/assets/dataset.png" alt="VisCoT Demo" width="500"/>
</p>